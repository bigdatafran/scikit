
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>1. Algoritmo Neareast Neighbors (KNN) &#8212; Trabajando con scikit learn</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Support Vector Machines (SVM)." href="supportVectorMachine.html" />
    <link rel="prev" title="Introducción" href="../introduccion.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Trabajando con scikit learn</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../introduccion.html">
                    Introducción
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Apredizaje supervisado
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Método Neareast Neighbors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="supportVectorMachine.html">
   2. Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RegresionLogistica/RegresionLogisticaEjemplo.html">
   3. Regresión Logística
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Softmax.html">
   4. Regresión Softmax
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="decisiontree/decisiontree.html">
   5. Árboles de decisión (Decision tree)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multiclass.html">
   6. Predicción con multiclases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ensembles.html">
   7. Métodos ensembles
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Apredizaje NO supervisado
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../nosupervisados/preliminares.html">
   8. Conceptos preliminares
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Otras herramientas
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../otros/pipelines.html">
   9. Pipelines o canalizaciones
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../otros/Regularizacion.html">
   10. Regularización de datos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../otros/ValidacionModelos.html">
   11. Validación de modelos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../otros/MedidasBondadAjuste.html">
   12. Medias de Bonda del Ajuste
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Índice de términos
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../genindex.html">
   13. Índice de términos
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bibliografía
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../otros/bibliografia.html">
   14. Bibliografia
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/supervisados/Neareast_Neighbors.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/supervisados/Neareast_Neighbors.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduccion">
   1.1. Introducción.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metodos-no-supervisados">
   1.2. Métodos no supervisados
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clases-kdtree-y-balltree">
     1.2.1. Clases KDTree y BallTree.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metodos-supervisados">
   1.3. Métodos supervisados.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#como-funciona-este-algoritmo">
     1.3.1. ¿ Cómo funciona este algoritmo?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distintas-clases-de-distancias-utilizadas">
     1.3.2. Distintas clases de distancias utilizadas.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#otras-distancias">
     1.3.3. Otras distancias.
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#distancia-de-minkowski">
       1.3.3.1. Distancia de Minkowski:
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#similitud-y-distancia-de-coseno">
       1.3.3.2. similitud y distancia de coseno.
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variantes-al-sistema-de-clasificacion-general">
     1.3.4. Variantes al sistema de clasificación general.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#como-seleccionar-k">
     1.3.5. Cómo seleccionar k.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementacion-el-algoritmo-en-python">
     1.3.6. Implementación el algoritmo en python.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clasificacion-con-nearest-neighbors">
     1.3.7. Clasificación con Nearest Neighbors.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#otro-ejemplo-practico">
     1.3.8. Otro ejemplo práctico.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uso-de-gridsearchcv">
   1.4. Uso de GridSearchCV.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regresion-nearest-neighbors">
   1.5. Regresión Nearest Neighbors
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Algoritmo Neareast Neighbors (KNN)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduccion">
   1.1. Introducción.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metodos-no-supervisados">
   1.2. Métodos no supervisados
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clases-kdtree-y-balltree">
     1.2.1. Clases KDTree y BallTree.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metodos-supervisados">
   1.3. Métodos supervisados.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#como-funciona-este-algoritmo">
     1.3.1. ¿ Cómo funciona este algoritmo?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distintas-clases-de-distancias-utilizadas">
     1.3.2. Distintas clases de distancias utilizadas.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#otras-distancias">
     1.3.3. Otras distancias.
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#distancia-de-minkowski">
       1.3.3.1. Distancia de Minkowski:
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#similitud-y-distancia-de-coseno">
       1.3.3.2. similitud y distancia de coseno.
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variantes-al-sistema-de-clasificacion-general">
     1.3.4. Variantes al sistema de clasificación general.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#como-seleccionar-k">
     1.3.5. Cómo seleccionar k.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementacion-el-algoritmo-en-python">
     1.3.6. Implementación el algoritmo en python.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clasificacion-con-nearest-neighbors">
     1.3.7. Clasificación con Nearest Neighbors.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#otro-ejemplo-practico">
     1.3.8. Otro ejemplo práctico.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uso-de-gridsearchcv">
   1.4. Uso de GridSearchCV.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regresion-nearest-neighbors">
   1.5. Regresión Nearest Neighbors
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="algoritmo-neareast-neighbors-knn">
<h1><span class="section-number">1. </span>Algoritmo Neareast Neighbors (KNN)<a class="headerlink" href="#algoritmo-neareast-neighbors-knn" title="Permalink to this headline">#</a></h1>
<section id="introduccion">
<h2><span class="section-number">1.1. </span>Introducción.<a class="headerlink" href="#introduccion" title="Permalink to this headline">#</a></h2>
<p id="index-0"><a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors">sklearn.neighbors</a> proporciona funcionalidad para métodos de aprendizaje tanto supervisados como no supervisados basados en la información que proporcionan los vecinos más próximos.el método basado en los vecinos más cercanos no supervisados son la base de muchos otros métodos de aprendizaje, especialmente el aprendizaje múltiple y la agrupación espectral. El aprendizaje basado en vecinos supervisados tiene dos variantes: clasificación para datos con etiquetas discretas y regresión para datos con etiquetas continuas.</p>
<p>El principio en el que se basan los métodos del vecino más cercano es encontrar un número predefinido de muestras de entrenamiento más cercanas en distancia al nuevo punto, y predecir la etiqueta a partir de ellas. El número de muestras puede ser una constante definida por el usuario (aprendizaje del vecino más cercano k), o variar en función de la densidad local de puntos (aprendizaje del vecino basado en el radio). La distancia puede ser, en general, cualquier medida métrica: la distancia euclidiana estándar es la opción más común. Los métodos basados en los vecinos se conocen como métodos de aprendizaje automático no generalizadores, ya que simplemente “recuerdan” todos sus datos de entrenamiento (posiblemente transformados en una estructura de indexación rápida, como un árbol de bolas (<a class="reference external" href="https://scikit-learn.org/stable/modules/neighbors.html#ball-tree">Ball Tree</a>) o un árbol KD (<a class="reference external" href="https://scikit-learn.org/stable/modules/neighbors.html#kd-tree">KD tree</a>)).</p>
<p>A pesar de su simplicidad, los vecinos más cercanos han tenido éxito en un gran número de problemas de clasificación y regresión, incluyendo dígitos escritos a mano y escenas de imágenes de satélite. Al ser un método no paramétrico, suele tener éxito en situaciones de clasificación en las que el límite de decisión es muy irregular.</p>
<p>La clase  sklearn.neighbors puede manejar matrices NumPy o scipy.sparse como entrada. Para las matrices densas, se admite un gran número de métricas de distancia posibles. Para las matrices dispersas, se admiten métricas de Minkowski arbitrarias para las búsquedas.</p>
<p>Para ver las diferencias entre KD tree y Ball Tree, <a class="reference external" href="https://towardsdatascience.com/tree-algorithms-explained-ball-tree-algorithm-vs-kd-tree-vs-brute-force-9746debcd940">visitar este enlace</a>.</p>
<p>A pesar de que estos métodos basados en la información que facilitan los vecinos más cercanos, se han incluido en el gran apartado dedicado al aprendizaje supervisado, dada que esta clase agrupa tanto métodos supervisados como no supervisado, se ha creído conveniente incluir los dos grandes tipos de aprendizaje en este capítulo, con la finalidad de ver las posibilidades que ofrece la clase sklearn.neighbors de Scikit Learn. Comenzamos con los métodos no supervisados.</p>
</section>
<section id="metodos-no-supervisados">
<h2><span class="section-number">1.2. </span>Métodos no supervisados<a class="headerlink" href="#metodos-no-supervisados" title="Permalink to this headline">#</a></h2>
<p>La clase <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors">NearestNeighbors</a> implementa el aprendizaje no supervisado de los vecinos más cercanos. Actúa como una interfaz uniforme para tres diferentes algoritmos de vecinos más cercanos: BallTree, KDTree, y un algoritmo de fuerza bruta basado en las rutinas de sklearn.metrics.pairwise. La elección del algoritmo de búsqueda de vecinos se controla a través del parámetro ‘algorithm’, que debe ser un valor dentro  de los siguientes: [‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’]. Cuando se pasa el valor por defecto ‘auto’, el algoritmo intenta determinar la mejor aproximación a partir de los datos de entrenamiento. Para una discusión de los puntos fuertes y débiles de cada opción, <a class="reference external" href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms">vea Algoritmos de Vecino más Cercano</a>.</p>
<p>A continuación se muestra un ejemplo de utilización de este método.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="c1"># n_neighbors indica el número de vecinos más cercanos</span>
<span class="c1"># Que queremos localizar para cada punto</span>
<span class="n">nbrs</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;ball_tree&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Índices: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">indices</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Distancias: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">distances</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Índices: 
 [[0 1 2]
 [1 0 2]
 [2 1 0]
 [3 4 5]
 [4 3 5]
 [5 4 3]
 [6 5 4]]

Distancias: 
 [[0.         1.         2.23606798]
 [0.         1.         1.41421356]
 [0.         1.41421356 2.23606798]
 [0.         1.         2.23606798]
 [0.         1.         1.41421356]
 [0.         1.41421356 2.23606798]
 [0.         4.47213595 5.83095189]]
</pre></div>
</div>
</div>
</div>
<p>En la primera salida, se muestran los puntos ( en este caso 3 porque así lo hemos indicado en la utilización del algoritmo) más cercanos entre sí.</p>
<p>En la segunda salida, se muestran las distancias del primer punto al resto. POr eso el primero valor vale cero, porque es la distancia de un punto a él mismo.</p>
<p>Como el conjunto de consulta coincide con el conjunto de entrenamiento, el vecino más cercano de cada punto es el propio punto, a una distancia de cero.</p>
<p>También es posible producir eficazmente un gráfico disperso que muestre las conexiones entre los puntos vecinos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors_graph</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 1., 1., 0., 0., 0., 0.],
       [1., 1., 1., 0., 0., 0., 0.],
       [1., 1., 1., 0., 0., 0., 0.],
       [0., 0., 0., 1., 1., 1., 0.],
       [0., 0., 0., 1., 1., 1., 0.],
       [0., 0., 0., 1., 1., 1., 0.],
       [0., 0., 0., 0., 1., 1., 1.]])
</pre></div>
</div>
</div>
</div>
<p>El conjunto de datos está estructurado de manera que los puntos cercanos en el orden de los índices están cerca en el espacio de los parámetros, lo que conduce a una matriz aproximadamente diagonal de bloques de los vecinos más cercanos. Este tipo de gráfico disperso es útil en una variedad de circunstancias que hacen uso de las relaciones espaciales entre los puntos para el aprendizaje no supervisado: en particular, véase <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap">Isomap</a>, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding">LocallyLinearEmbedding</a> y <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering">SpectralClustering</a>.</p>
<section id="clases-kdtree-y-balltree">
<h3><span class="section-number">1.2.1. </span>Clases KDTree y BallTree.<a class="headerlink" href="#clases-kdtree-y-balltree" title="Permalink to this headline">#</a></h3>
<p id="index-1">Como alternativa, se pueden utilizar las clases KDTree o BallTree directamente para encontrar los vecinos más cercanos. Esta es la funcionalidad que envuelve la clase NearestNeighbors utilizada anteriormente. El Ball Tree y el KDTree tienen la misma interfaz; aquí mostraremos un ejemplo de uso del KDTree:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KDTree</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">kdt</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="n">kdt</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_distance</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0, 1],
       [1, 0],
       [2, 1],
       [3, 4],
       [4, 3],
       [5, 4]], dtype=int64)
</pre></div>
</div>
</div>
</div>
<p>Consulte la documentación de las clases <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree">KDTree</a> y <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree">BallTree</a> para obtener más información sobre las opciones disponibles para las búsquedas de vecinos más cercanos, incluida la especificación de estrategias de consulta, métricas de distancia, etc. Para obtener una lista de métricas disponibles, consulte la documentación de la clase <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric">DistanceMetric</a>.</p>
</section>
</section>
<section id="metodos-supervisados">
<h2><span class="section-number">1.3. </span>Métodos supervisados.<a class="headerlink" href="#metodos-supervisados" title="Permalink to this headline">#</a></h2>
<p>K-Nearest Neighbours ( normalmente denominado KNN) es un algoritmo  de aprendizaje automático sencillo pero muy utilizado en la práctica. En los métodos supervisados, se puede usar tanto para la clasificación de un elemento como para hacer modelos de regresión.  KNN agrupa los datos en clusters o subconjuntos coherentes y clasifica los nuevos datos introducidos en función de su similitud con los datos previamente entrenados.</p>
<p>El dato de entrada se asigna a la clase con la que comparte el mayor número de vecinos más cercanos ( de aquí viene su nombre).</p>
<p>El algoritmo K-Nearest-Neighbours, es un algoritmo del que  se puede decir que es un algoritmo de clasificación no paramétrico, es decir, no hace ninguna presunción sobre la distribución del conjunto de datos base sobre el que se entrena el modelo.</p>
<p>Al algoritmo KNN también se le denomina <strong>algoritmo de aprendizaje perezoso ( lazy learner algorithm)</strong> porque no aprende del conjunto de entrenamiento inmediatamente, sino que almacena el conjunto de datos y, cuando se va a hecer la clasificación, realiza la acción necesaria para resolver el problema en el conjunto de datos. En la fase de entrenamiento sólo se encarga de almacenar los datos que se utilizan para entrenar ( motivo por el cual requiere una gran cantidad de memoria para volúmenes importantes de datos) y cuando obtiene nuevos datos, los clasifica en una categoría que es muy similar a los nuevos datos.</p>
<p>Un ejemplo de aplicación de este tipo de procedimientos puede ser el siguiente. Según datos históricos de una determinada entidad bancaria, se podrían clasificar los clientes en base a morosos o no morosos, en base a los diversos conjuntos de datos almacenados, previamente. Si aparece un nuevo cliente a solicitar un préstamos, se le pueden solicitar los datos de las variables que han servido para elaborar el modelo, y en base la predicción realizada con esta metodología, se le podría tratar de una u otra manera.</p>
<p>En la siguiente figura se puede ver de forma gráfica lo comentado en estas líneas.</p>
<p><img alt="K_3.svg" src="supervisados/attachment:K_3.svg" /></p>
<p>En la siguiente tabla, se presentan las ventajas e inconvenientes más significativos de esta tecnología.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Ventajas</p></th>
<th class="head"><p>Inconvenientes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Fácil de implementar</p></td>
<td><p>Hay que seleccionar bien el valor k de vecinos a usar</p></td>
</tr>
<tr class="row-odd"><td><p>No necesita suponer distribución de los datos</p></td>
<td><p>La fase de clasificación es lenta</p></td>
</tr>
<tr class="row-even"><td><p>La fase de entrenamientos suele ser rápida</p></td>
<td><p>No admite datos ausentes</p></td>
</tr>
</tbody>
</table>
<p>Como los métodos que utilizan este tipo de algoritmos están basados en las distancias entre los punto, en el siguiente enlace se pueden ver algunas cuestiones muy interesantes sobre las mismas y en relación con este algoritmo.</p>
<p><a class="reference external" href="https://ichi.pro/es/tipos-de-metricas-de-distancia-y-uso-de-metricas-de-distancia-definidas-por-el-usuario-en-el-algoritmo-knn-de-172716170298257">https://ichi.pro/es/tipos-de-metricas-de-distancia-y-uso-de-metricas-de-distancia-definidas-por-el-usuario-en-el-algoritmo-knn-de-172716170298257</a></p>
<section id="como-funciona-este-algoritmo">
<h3><span class="section-number">1.3.1. </span>¿ Cómo funciona este algoritmo?<a class="headerlink" href="#como-funciona-este-algoritmo" title="Permalink to this headline">#</a></h3>
<p>Con este procedimiento de clasificación, lo que se busca es obtener un voto mayoritario de pertenencia a una clase, emitido por los vecinos más próximos, los cuales se determinan mediante una adecuada función de distancia.</p>
<p>En consecuencia de lo que se trata es de lo siguiente: se tienen una serie de instancias observadas anteriormente, y cada una de esas instancias conformadas por una serie de observaciones más otra variable de clasificación. Entonces dada una nueva instancia que no se conoce a la clase a la que pertenece, pero sí los valores que tienen el resto de las variables independientes, lo que se trata es calcular o inferir a que clase pertenece. Para conseguir este objetivo lo que se buscan son lo k vecinos más próximos, y entonces utilizando un sistema de votación mayoritario, se asignaría  esa nueva observación a la clase más votada.</p>
<p>Veamos esto con un simple ejemplo utilizando unos pocos datos que figuran en el fichero excel que se ha elaborado al efecto y que se muestran en la siguiente figura.</p>
<p><img alt="excellKNN.PNG" src="supervisados/attachment:excellKNN.PNG" /></p>
<p>En el ejemplo ficticio anterior, se han colocado en la hoja excell un total de 12 personas, las primeras once tienen datos de edad e ingresos ( en miles de euros) y la clase a la que pertenecen ( columna asig) . La duodécima persona, tan sólo tiene datos de edad e ingresos y se quiere inferir cual es la clase más probable a la que pertenece. Para obtener este resultado, se ha calculado la distancia euclidea que hay entre el valor que queremos clasificar y las otras 11 observaciones ( tomando los valores de la variables edad e ingreso). En la columna “Dist. Euclidea”, se han anotado estas distancias. Si empleamos el criterio de los tres vecinos más próximos, que se han señalado en el gráfico anterior con sendas flechas, podemos ver que uno votaría N y los otros dos Y, por lo tanto mediante este sistema de votación mayoritario, el último dato, se clasificaría en la categoría Y. Sin embargo, el problema se tiene si se utiliza k = 4 pues en este caso se producirá un empate.</p>
<p>Para evitar este problema de empates, se suele utilizar como criterio, intentar utilizar un valor de k impar y en el caso de utilizar un valor par y producirse este empate, entonces se suelen utilizar normas heurísticas para deshacer este empate, como por ejemplo seleccionar la clase que contiene al vecino más próximo, seleccionar la clase con la distancia media menor, etc.</p>
<p>Como ya se ha comentado al comienzo de este apartado, el algoritmo KNN también se utiliza para resolver problemas de  regresión, aunque en menor medida que para clasificar, en este caso, en lugar de elegir la clase más votada entre los k puntos más cercanos, lo que se hace es calcular la media o mediana de las respuestas ( o variable dependiente) que tienen los k puntos más cercanos al que se quiere predecir el valor de la regresión.</p>
</section>
<section id="distintas-clases-de-distancias-utilizadas">
<h3><span class="section-number">1.3.2. </span>Distintas clases de distancias utilizadas.<a class="headerlink" href="#distintas-clases-de-distancias-utilizadas" title="Permalink to this headline">#</a></h3>
<p>Como ya se ha visto un elemento muy importante que influye es la decisión final de este clasificador es la distancia utilizada. En este sentido se puede decir que en función de los atributos con los que se trabaja, las distancias más utilizada son la euclídea, si los datos son numéricos, y la segunda se utiliza si los atributos son de tipo nominal o binarios.</p>
<p id="index-2">Veamos en primer lugar cómo definir la <strong>distancia euclídea</strong>. Supongamos que disponemos de dos vectores numéricos <span class="math notranslate nohighlight">\(x=(x_1,x_2,...,x_n)\)</span> e <span class="math notranslate nohighlight">\(y=(y_1,y_2,...,y_n)\)</span>, entonces se define la distancia o norma euclídea como:</p>
<div class="math notranslate nohighlight">
\[d(x,y) = \sum_{i=1}^{n}(x_{i}-y_{i})^{2}\]</div>
<p id="index-3">Por lo que respecta a la <strong>distancia de Hamming</strong>, se define de la siguiente manera:</p>
<div class="math notranslate nohighlight">
\[dh(y,x_{i})=\sum_{j=1}^{k}\delta(x_{j},x_{ij}) \]</div>
<p>Donde la distancia individual <span class="math notranslate nohighlight">\(\delta\)</span>, se define como:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta(x_{j},x_{ij})=\begin{cases}
1 &amp; si\ x_{j}=x_{ij}\\0 &amp; si \ x_{j}\neq x_{ij} \end{cases}\end{split}\]</div>
<p>En python se puede calcular la distancia de Hamming, utilizando la biblioteca scipy.spatial.distance. Para ver cómo se pude obtener su valor, hay que tener presente inicialmente que este método devuelve el porcentaje de elementos coincidentes de dos matrices, por lo tanto para obtener el número de elementos coincidentes ( que es lo que realmente mide la distancia de Haamming), lo que hay que hacer es multiplicar por el número total de elementos de las dos matrices. A continuación se puede ver un sencillo ejemplo, sobre su cálculo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">hamming</span>

<span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">hamming</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.0
</pre></div>
</div>
</div>
</div>
<p><strong>NOTA</strong>: Esta distancia de Hamming se puede utilizar también para comparar dos cadenas de caracteres y ver si las mismas son similares o difieren sustancialmente. La biblioteca de Python anteriormente referenciada también permite calcular esta distancia para dos matrices que contienen caracteres individuales, como se puede ver a continuación.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#definimos dos matrices de caracteres</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">]</span>

<span class="c1">#calculamos la distancia de Hamming entre las dos matrices</span>
<span class="c1">#es decir calculamos el número de caracteres donde difieren</span>
<span class="n">hamming</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span> <span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="otras-distancias">
<h3><span class="section-number">1.3.3. </span>Otras distancias.<a class="headerlink" href="#otras-distancias" title="Permalink to this headline">#</a></h3>
<p>A pesar de que las distancias comentadas en los párrafos anteriores, son las más comunes y utilizadas en la práctica diaria, no son las únicas, y a continuación se va a proceder a exponer alguna más muy utilizada en la ciencia estadística en general.</p>
<section id="distancia-de-minkowski">
<h4><span class="section-number">1.3.3.1. </span>Distancia de Minkowski:<a class="headerlink" href="#distancia-de-minkowski" title="Permalink to this headline">#</a></h4>
<p id="index-4">La fórmula matemática que caracteriza esta distancia es la siguiente:</p>
<div class="math notranslate nohighlight">
\[d(x,y)=\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p}\right)^{1/p}\]</div>
<p>Cuando se tiene que p = 1, entonces se obtiene la fórmula que caracteriza a la <strong>distancia de Manhattan</strong>:</p>
<div class="math notranslate nohighlight">
\[dis.\ Manhattan=\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|\right)\]</div>
</section>
<section id="similitud-y-distancia-de-coseno">
<h4><span class="section-number">1.3.3.2. </span>similitud y distancia de coseno.<a class="headerlink" href="#similitud-y-distancia-de-coseno" title="Permalink to this headline">#</a></h4>
<p id="index-5">Con esta métrica se intenta medir la similitud que hay entre dos vectores de datos, midiendo para ello el coseno que forman esos dos vectores. Entonces si el ángulo es cero ( si los dos son similares) entonces el valor del coseno es 1 y en este caso se tendrá que las dos observaciones son muy similares. Esta similitud se define de la siguiente forma:</p>
<div class="math notranslate nohighlight">
\[similitud=\frac{A\cdot B}{\left\Vert A\right\Vert \cdot\left\Vert B\right\Vert }=\frac{\sum A_{i}\cdot B_{i}}{\sqrt{\sum A_{i}^{2}}\cdot\sqrt{\sum B_{i}^{2}}}\]</div>
<p>Y entonces en base a esa medida de similitud, se define la <strong>distancia coseno como 1-similitud coseno</strong>.</p>
<p>Scikit learn utiliza el método KNeighborsClassifier para hacer este tipo de clasificaciones, y entre sus parámetros se encuentra “metric” que permite definir la métrica que se quiere utilizar. Estas posibles distancias, se <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric">pueden ver en este enlace</a>.</p>
<p>Pero también se pueden definir funciones para definir distancias no contempladas en la implementación de Scikit Learn, como por ejemplo la que se define con la función que se puede ver en el siguiente código:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">funcion_distancia</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Definición de una distancia en python</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">yi</span><span class="p">)</span>
        <span class="n">max_val</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">yi</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">min_val</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">min_val</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">max_val</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">min_val</span> <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="n">min_val</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">max_val</span> <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="n">max_val</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">total</span>

<span class="n">funcion_distancia</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.5
</pre></div>
</div>
</div>
</div>
<p>Entonces si yo quisiera utilizar esa función de distancia dentro de Scikip learn, no tendría más que utilizar el siguiente código:</p>
<code>
 knn = KNeighborsClassifier(n_neighbors=3, algorithm='ball_tree', metric = funcion_distancia)   
</code>
<p>El psudocódigo que se utilizaría para este algoritmo sería el siguiente:</p>
<pre>
    COMIENZO
    Entrada: D = {(x1,y1),(x2,y2),...,(xn,yn)}
        x = nuevo objeto a clasificar
    Para todo objeto ya clasificado (xi,yi)
        calcular d(x,xi)
    Ordenar forma ascendente di ( i =1,2,...n)
    Nos quedamos con los k casos ya clasificados más cercanos a x
    Asignar x a la clase más frecuente de los k casos localizados en paso anterior
    FIN
</pre>
</section>
</section>
<section id="variantes-al-sistema-de-clasificacion-general">
<h3><span class="section-number">1.3.4. </span>Variantes al sistema de clasificación general.<a class="headerlink" href="#variantes-al-sistema-de-clasificacion-general" title="Permalink to this headline">#</a></h3>
<p>Existen algunas variantes a este algoritmo que intentan mejorar el sistema de clasificación general ya comentado anteriormente. Estas variantes, se puede ver en el siguiente enlace:</p>
<p><a class="reference external" href="http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t9knn.pdf">http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t9knn.pdf</a></p>
</section>
<section id="como-seleccionar-k">
<h3><span class="section-number">1.3.5. </span>Cómo seleccionar k.<a class="headerlink" href="#como-seleccionar-k" title="Permalink to this headline">#</a></h3>
<p>No existe un criterio único, para seleccionar el “mejor valor de k”, pero si existen diversos criterios que pueden ayudar a la hora de tomar esta decisión. Uno de estos criterios puede ser calcular la precisión del modelo (para los valores de prueba) para diferentes valores de k y hacer un gráfico que cruce el valor de k y la precisión alcanzada y elegir el primer k que maximice esos valores. Para obtener la precisión de un modelo, se puede utilizar la propiedad “score()” de scikit learn. A continuación se incluyen unas pocas líneas de código que muestran cómo poder hacerlo:</p>
<code>
    knn = KNeighborsClassifier(n_neighbors)
    ......
    knn.score(X_test, y_test)
</code>
<p>Otra posibilidad es utilizar curvas de error, tanto para los valores de entrenamiento como de prueba y observar las curvas obtenidas. Un ejemplo de esto lo podemos ver en la siguiente figura:</p>
<p><img alt="valor_k.PNG" src="supervisados/attachment:valor_k.PNG" /></p>
<p>Como podemos ver en esta figura, un valor bajo de k implicaría un sobreajuste  y una alta variabilidad, en este caso el error de entrenamiento es muy bajo, pero en el test sube considerablemente. Tener presente que si k=1 en el caso de entrenamiento el error siempre es cero porque el único punto que se elije como vecino más próximo es el mismo punto.</p>
<p>A medida que k aumenta, el error del test va disminuyendo hasta un determinado valor de k, a partir del cual el error vuelve a aumentar. Entonces se puede aceptar un valor de k en el cual el cual error del test se estabiliza  y se hace mínimo. En el ejemplo anterior se podría aceptar <a class="reference external" href="http://k=8.No">k=8.No</a> obstante y para evitar tomar decisiones de empates de los datos, conviene tomar un valor de k impar.</p>
<p>Un posible código para implementar este tipo de gráficas podría ser el siguiente:</p>
<code>
error1= []
error2= []
for k in range(1,15):
    knn= KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train,y_train)
    y_pred1= knn.predict(X_train)
    error1.append(np.mean(y_train!= y_pred1))
    y_pred2= knn.predict(X_test)
    error2.append(np.mean(y_test!= y_pred2))
# plt.figure(figsize(10,5))
plt.plot(range(1,15),error1,label="train")
plt.plot(range(1,15),error2,label="test")
plt.xlabel('k Value')
plt.ylabel('Error')
plt.legend()    
</code>
</section>
<section id="implementacion-el-algoritmo-en-python">
<h3><span class="section-number">1.3.6. </span>Implementación el algoritmo en python.<a class="headerlink" href="#implementacion-el-algoritmo-en-python" title="Permalink to this headline">#</a></h3>
<p>Como ya se ha comentado anteriormente, este algoritmo no es difícil de implementar y por lo tanto se podrían escribir sin mucha dificultad una serie de líneas de código que nos  faciliten la solución de este problema sin necesidad de utilizar la librería de Scikit Learn.</p>
<p>Una posible implementación del mismo se puede ver en los siguientes enlaces:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://openaccess.uoc.edu/webapps/o2/bitstream/10609/90946/6/jaluquegTFM0219memoria.pdf">http://openaccess.uoc.edu/webapps/o2/bitstream/10609/90946/6/jaluquegTFM0219memoria.pdf</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761">https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761</a></p></li>
</ul>
</section>
<section id="clasificacion-con-nearest-neighbors">
<h3><span class="section-number">1.3.7. </span>Clasificación con Nearest Neighbors.<a class="headerlink" href="#clasificacion-con-nearest-neighbors" title="Permalink to this headline">#</a></h3>
<p>La clasificación basada en los vecinos próximos es un tipo de aprendizaje basado en instancias o aprendizaje no generalizador: no intenta construir un modelo interno general, sino que simplemente almacena instancias de los datos de entrenamiento. La clasificación se calcula a partir de una simple votación mayoritaria de los vecinos más cercanos de cada punto: a un punto de consulta se le asigna la clase de datos que tiene más representantes dentro de los vecinos más cercanos del punto.</p>
<p>scikit-learn implementa dos clasificadores diferentes de vecinos más cercanos: <strong>KNeighborsClassifier</strong> implementa el aprendizaje basado en los k vecinos más cercanos de cada punto de consulta, donde k es un valor entero especificado por el usuario. <strong>RadiusNeighborsClassifier</strong> implementa el aprendizaje basado en el número de vecinos dentro de un radio fijo r de cada punto de entrenamiento, donde r es un valor de punto flotante especificado por el usuario.</p>
<p>La clasificación por vecinos en K-NeighborsClassifier es la técnica más utilizada. La elección óptima del valor k depende en gran medida de los datos: en general, un valor mayor suprime los efectos del ruido, pero hace que los límites de la clasificación sean menos nítidos.</p>
<p>En los casos en los que los datos no están muestreados uniformemente, la clasificación de vecinos basada en radios en RadiusNeighborsClassifier puede ser una mejor opción. El usuario especifica un radio fijo r, de manera que los puntos de los vecindarios más dispersos utilizan menos vecinos más cercanos para la clasificación. Para los espacios de parámetros de alta dimensión, este método se vuelve menos eficaz debido a la llamada “maldición de la dimensionalidad”.</p>
<p>La clasificación básica de vecinos más cercanos utiliza ponderaciones uniformes: es decir, el valor asignado a un punto de consulta se calcula a partir de un voto mayoritario simple de los vecinos más cercanos. <em>En algunas circunstancias, es mejor ponderar los vecinos de forma que los más cercanos contribuyan más al ajuste</em>. Esto puede lograrse mediante la palabra clave weights. El valor por defecto, weights = ‘uniform’, asigna pesos uniformes a cada vecino. weights = ‘distance’ asigna pesos proporcionales a la inversa de la distancia al punto de consulta. Alternativamente, se puede proporcionar una función definida por el usuario de la distancia para calcular los pesos.</p>
<p>A continuación se muestra un ejemplo con estas diferencias en pesos</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">datasets</span>

<span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c1"># import some data to play with</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># we only take the first two features. We could avoid this ugly</span>
<span class="c1"># slicing by using a two-dim dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">h</span> <span class="o">=</span> <span class="mf">0.02</span>  <span class="c1"># step size in the mesh</span>

<span class="c1"># Create color maps</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="s2">&quot;cyan&quot;</span><span class="p">,</span> <span class="s2">&quot;cornflowerblue&quot;</span><span class="p">])</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;darkorange&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;darkblue&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">weights</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="s2">&quot;distance&quot;</span><span class="p">]:</span>
    <span class="c1"># we create an instance of Neighbours Classifier and fit the data.</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Plot the decision boundary. For that, we will assign a color to each</span>
    <span class="c1"># point in the mesh [x_min, x_max]x[y_min, y_max].</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

    <span class="c1"># Put the result into a color plot</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>

    <span class="c1"># Plot also the training points</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">hue</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">y</span><span class="p">],</span>
        <span class="n">palette</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span>
        <span class="s2">&quot;3-Class classification (k = </span><span class="si">%i</span><span class="s2">, weights = &#39;</span><span class="si">%s</span><span class="s2">&#39;)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neareast_Neighbors_15_0.png" src="../_images/Neareast_Neighbors_15_0.png" />
<img alt="../_images/Neareast_Neighbors_15_1.png" src="../_images/Neareast_Neighbors_15_1.png" />
</div>
</div>
</section>
<section id="otro-ejemplo-practico">
<h3><span class="section-number">1.3.8. </span>Otro ejemplo práctico.<a class="headerlink" href="#otro-ejemplo-practico" title="Permalink to this headline">#</a></h3>
<p>En este apartado se va a proceder a realizar un ejemplo práctico, sobre el uso de esta herramienta con la biblioteca de Python Scikit learn. En primer lugar es preciso matizar que desde esta biblioteca y con el módulo denominado sklearn.neighbors, se pueden trabajar tanto en modelos supervisados, como con modelos no supervisados. Un resumen de todos estos modelos, se pueden <a class="reference external" href="https://scikit-learn.org/stable/modules/neighbors.html">ver en este enlace</a>.</p>
<p>Estos últimos modelos no son objeto de este tema, pero para el lector interesado, se puede decir que existen para ello tres algoritmos desarrollados en Scikit learn: BallTree, KDTree y el denomiando de fuerza bruta.</p>
<p>Respecto de los algoritmos supervisados, se puede decir que se pueden utilizar tanto para regresión como para clasificación. Para la regresión Scikit learn ofrece dos implementaciones: <em>KNeighborsRegressor</em> y  <em>RadiusNeighborsRegressor</em>.</p>
<p>Lo que interesa realmente en este apartado es la posibilidad de utilizar el algoritmo KNN para hacer clasificaciones, para ello Scikit learn ofrece dos métodos diferentes:  <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier">KNeighborsClassifier</a> ( que sería el que implementa el método desarrollado en apartados anteriores), y también <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier">RadiusNeighborsClassifier</a>, que se basa en utilizar los vecinos más próximos dentro de un determinado radio.</p>
<p>A continuación procedemos a desarrollar un ejemplo comprensivo de las posibilidades con las que podemos operar con Scikip learn.</p>
<p>En primer lugar crearemos una serie de puntos con la clase <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">make_blobs</a> con dos conjuntos posibles bien distinguidos.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">n_puntos</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">semilla</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_puntos</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">semilla</span><span class="p">,</span> 
                  <span class="n">centers</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span><span class="n">cluster_std</span> <span class="o">=</span> <span class="mf">3.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Veamos a continuación cómo están distribuidos los puntos generados, distinguiendo por un color diferente la pertenencia a una u otra clase.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x18634b3e640&gt;
</pre></div>
</div>
<img alt="../_images/Neareast_Neighbors_19_1.png" src="../_images/Neareast_Neighbors_19_1.png" />
</div>
</div>
<p>Procedemos a continuación a normalizar los valores, a fin de evitar problemas derivados de la escala en la clasificación.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># comprobamos que la media es cero y la varianza 1</span>
<span class="n">X_scaled</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([6.53921362e-16, 2.83106871e-17])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_scaled</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1., 1.])
</pre></div>
</div>
</div>
</div>
<p>Ahora procedemos a obtener los datos de entrenamiento y los datos de test, mediante el siguiente código.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># comprobemos se han obtenido el 80% para train</span>
<span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>80.0
</pre></div>
</div>
</div>
</div>
<p>Ahora ya podemos aplicar la clase KNeighborsClassifier, pero previamente vamos a ver cual es el valor de k más adecuado. Lo haremos con dos procedimietos:</p>
<p>El primero consiste en calcular los errores cometidos tanto en la muestra de entrenamiento como de prueba.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span>  <span class="n">KNeighborsClassifier</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">error1</span><span class="o">=</span> <span class="p">[]</span>
<span class="n">error2</span><span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">15</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trabajando para k=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="n">knn</span><span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred1</span><span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">error1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span><span class="o">!=</span> <span class="n">y_pred1</span><span class="p">))</span>
    <span class="n">y_pred2</span><span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">error2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span><span class="o">!=</span> <span class="n">y_pred2</span><span class="p">))</span>
<span class="c1"># plt.figure(figsize(10,5))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">15</span><span class="p">),</span><span class="n">error1</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">15</span><span class="p">),</span><span class="n">error2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;k Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trabajando para k=1
Trabajando para k=2
Trabajando para k=3
Trabajando para k=4
Trabajando para k=5
Trabajando para k=6
Trabajando para k=7
Trabajando para k=8
Trabajando para k=9
Trabajando para k=10
Trabajando para k=11
Trabajando para k=12
Trabajando para k=13
Trabajando para k=14
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x18634b16550&gt;
</pre></div>
</div>
<img alt="../_images/Neareast_Neighbors_28_2.png" src="../_images/Neareast_Neighbors_28_2.png" />
</div>
</div>
<p>Como puede verse en este gráfico, el primer valor del conjunto test que minimiza el error sería para k= 7 que además es impar y será el valor con el que inicialmente podríamos trabajar.</p>
<p>Otro procedimiento consiste en aplicar el siguiente procedimiento:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>([&lt;matplotlib.axis.XTick at 0x186104652e0&gt;,
  &lt;matplotlib.axis.XTick at 0x18610465d60&gt;,
  &lt;matplotlib.axis.XTick at 0x18634736460&gt;,
  &lt;matplotlib.axis.XTick at 0x18634a04370&gt;,
  &lt;matplotlib.axis.XTick at 0x186349f5eb0&gt;],
 [Text(0, 0, &#39;&#39;),
  Text(0, 0, &#39;&#39;),
  Text(0, 0, &#39;&#39;),
  Text(0, 0, &#39;&#39;),
  Text(0, 0, &#39;&#39;)])
</pre></div>
</div>
<img alt="../_images/Neareast_Neighbors_30_1.png" src="../_images/Neareast_Neighbors_30_1.png" />
</div>
</div>
<p>Con este método también podemos ver que el valor de k que mejora el score es 7 que confirma lo que ya habíamos obtenido anteriormente. Por lo tanto procedemos a entrenar el modelo con el valor de k = 7.</p>
<p>Elegido el valor de k procedemos al entrenamiento del modelo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">valor_k</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">valor_k</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La acuracidad en el conjunto de entrenamiento es: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span>
        <span class="nb">format</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;La acuracidad en el conjunto de test es: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span>
        <span class="nb">format</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La acuracidad en el conjunto de entrenamiento es: 0.95
La acuracidad en el conjunto de test es: 0.90
</pre></div>
</div>
</div>
</div>
<p>Como puede verse, obtenemos una precisión de 95% para el conjunto de entrenamiento y del 90% para el test, lo que se puede considerar como un resultado bastante aceptable.</p>
<p>Obtengamos a continuación la matriz de confusión obtenida en base al modelo creado anteriormente.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La matriz de confusión es la siguiente:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">pred</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Los distintos valores de ajuste del modelo son:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La matriz de confusión es la siguiente:

[[19  1]
 [ 3 17]]
Los distintos valores de ajuste del modelo son:

              precision    recall  f1-score   support

           0       0.86      0.95      0.90        20
           1       0.94      0.85      0.89        20

    accuracy                           0.90        40
   macro avg       0.90      0.90      0.90        40
weighted avg       0.90      0.90      0.90        40
</pre></div>
</div>
</div>
</div>
<p>A continuación vamos a hacer un ejercicio de representación gráfica, para ver cómo quedan las regiones de clasificación del modelo, con lo cual nos facilita la interpretación de donde pueden caer las predicciones. Es preciso tener en cuenta que esta representación gráfica la podemos hacer porque sólo tenemos dos variables en las que basar la clasificación, si tuviéramos más sería más complejo hacer este tipo de representaciones e incluso la conclusiones se deberán hacer en base a los resultados numéricos del modelo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>

<span class="n">h</span> <span class="o">=</span> <span class="mf">0.02</span> <span class="c1"># es el paso de la rejilla</span>

<span class="c1">#Creamos los mapas de colores</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FFAAAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#ffcc99&#39;</span><span class="p">,</span> <span class="s1">&#39;#ffffb3&#39;</span><span class="p">,</span><span class="s1">&#39;#b3ffff&#39;</span><span class="p">,</span><span class="s1">&#39;#c2f0c2&#39;</span><span class="p">])</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#ff9933&#39;</span><span class="p">,</span><span class="s1">&#39;#FFFF00&#39;</span><span class="p">,</span><span class="s1">&#39;#00ffff&#39;</span><span class="p">,</span><span class="s1">&#39;#00FF00&#39;</span><span class="p">])</span>

<span class="c1"># Creamos una instancia del clasificador y ajustamos los datos</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">valor_k</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Dibujamos la frontera de decisión. Para ello asignamos un color to each</span>
<span class="c1"># a cada punto de la rejilla dependiendo donde corrsponde ese punto.</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

<span class="c1"># ponemos el resultado en el color que corresponde</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>

<span class="c1"># Dibujamos los puntos</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span>
                <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

<span class="n">patch0</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;1&#39;</span><span class="p">)</span>
<span class="n">patch1</span> <span class="o">=</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;#ff9933&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;2&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="p">[</span><span class="n">patch0</span><span class="p">,</span> <span class="n">patch1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Ejemplo clasificación binaria con (k = </span><span class="si">%i</span><span class="s2">)&quot;</span>
              <span class="o">%</span> <span class="p">(</span><span class="n">valor_k</span><span class="p">))</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neareast_Neighbors_36_0.png" src="../_images/Neareast_Neighbors_36_0.png" />
</div>
</div>
</section>
</section>
<section id="uso-de-gridsearchcv">
<h2><span class="section-number">1.4. </span>Uso de GridSearchCV.<a class="headerlink" href="#uso-de-gridsearchcv" title="Permalink to this headline">#</a></h2>
<p>En apartados anteriores, se ha utilizado código ac hoc  para determinar el valor del hiperparámetro k, pero scikit learn ofrece un método para poder hacer esto y que sirve no sólo para la clasificación KNN que se ha desarrollado anteriormente, sino también para cualquier otra herramienta de clasificación o regresión, pues lo que hace es repetir el procesamiento que se indique con la finalidad de encontrar los hiperparámetros que minimizan el error a la hora de hacer un determinado ajuste. Nos estamos refiriendo al método GridSearchCV, que se puede ver <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">en este enlace</a>.</p>
<p>En nuestro caso concreto, vamos utilizar esta facilidad que nos ofrece Scikit Learn a fin de localizar cual sería el mejor valor de K utilizando este método. Veamos esto con el siguiente ejemplo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">parametros</span> <span class="o">=</span><span class="p">{</span>
    <span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">KNeighborsClassifier</span><span class="p">(),</span>
    <span class="n">parametros</span><span class="p">,</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">resultados</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>En este sencillo ejemplo, tan sólo nos vamos a fijar en un sólo hiperperámetro de KNeighborsClassifier, que indica el número de vecinos más próximos a tener en cuenta ( existen más hiperparámetros para este metodo que se pueden ver en la documentación oficial de Scikit learn.</p>
<p>Para este caso concreto, definimos el único hiperparámetro que queremos ajustar mediante un diccionario python que se ha denominado parametros. Después creamos una instancia de GridSearchCV, y se le indica lo siguiente:</p>
<p>KNeighborsClassifier(). Para indicar que se utiliza este procedimeinto.</p>
<p>parametros. Es el diccionario previamente creado</p>
<p>cv=2. Para indicar que en sus pruebas utilice una cross-validación de tamaño 2</p>
<p>n_jobs = -1. Para indicar que utilice programación en paralelo y utilice todos los “procesadores” disponibles</p>
<p>Utilizando este procedimiento podemos usar su resultado para obtener diferentes valores optimizados de la herramienta que se quiere utilizar. Las posibles salidas se muestran a continuación.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Obtención del score: &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resultados</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Datos del estimador a utilizar&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resultados</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Mejor parámetro estimado: &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resultados</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Obtención del score: 
0.94375

Datos del estimador a utilizar
KNeighborsClassifier(n_neighbors=7)

Mejor parámetro estimado: 
{&#39;n_neighbors&#39;: 7}
</pre></div>
</div>
</div>
</div>
</section>
<section id="regresion-nearest-neighbors">
<h2><span class="section-number">1.5. </span>Regresión Nearest Neighbors<a class="headerlink" href="#regresion-nearest-neighbors" title="Permalink to this headline">#</a></h2>
<p>La regresión basada en los vecinos puede utilizarse en los casos en que las etiquetas de los datos son variables continuas y no discretas. La etiqueta asignada a un punto de consulta se calcula a partir de la media de las etiquetas de sus vecinos más cercanos.</p>
<p>scikit-learn implementa dos regresores de vecinos diferentes:</p>
<p>KNeighborsRegressor implementa el aprendizaje basado en los k vecinos más cercanos de cada punto de consulta, donde k es un valor entero especificado por el usuario.</p>
<p>RadiusNeighborsRegressor implementa el aprendizaje basado en los vecinos dentro de un radio fijo del punto de consulta, donde r es un valor de punto flotante especificado por el usuario.</p>
<p>La regresión básica de vecinos más cercanos utiliza ponderaciones uniformes: es decir, cada punto de la vecindad local contribuye uniformemente a la clasificación de un punto de consulta. En algunas circunstancias, puede ser ventajoso ponderar los puntos de manera que los puntos cercanos contribuyan más a la regresión que los puntos lejanos. Esto puede lograrse mediante la palabra clave pesos. El valor por defecto, weigths = ‘uniform’, asigna pesos iguales a todos los puntos. weights = ‘distance’ asigna pesos proporcionales a la inversa de la distancia al punto de consulta. Alternativamente, se puede proporcionar una función definida por el usuario de la distancia, que se utilizará para calcular los pesos.</p>
<p>Veamos a continuación un ejemplo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># #############################################################################</span>
<span class="c1"># Generate sample data</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">500</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1"># Add noise to targets</span>
<span class="n">y</span><span class="p">[::</span><span class="mi">5</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>

<span class="c1"># #############################################################################</span>
<span class="c1"># Fit regression model</span>
<span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">weights</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="s2">&quot;distance&quot;</span><span class="p">]):</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">y_</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;darkorange&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">y_</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;navy&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prediction&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;KNeighborsRegressor (k = </span><span class="si">%i</span><span class="s2">, weights = &#39;</span><span class="si">%s</span><span class="s2">&#39;)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neareast_Neighbors_42_0.png" src="../_images/Neareast_Neighbors_42_0.png" />
</div>
</div>
<p>El uso de vecinos más cercanos de múltiples salidas para la regresión se demuestra en el siguiente ejemplo.. En este ejemplo, las entradas X son los píxeles de la mitad superior de las caras y las salidas Y son los píxeles de la mitad inferior de esas caras.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_olivetti_faces</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_random_state</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>

<span class="c1"># Load the faces datasets</span>
<span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">fetch_olivetti_faces</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">targets</span> <span class="o">&lt;</span> <span class="mi">30</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">targets</span> <span class="o">&gt;=</span> <span class="mi">30</span><span class="p">]</span>  <span class="c1"># Test on independent people</span>

<span class="c1"># Test on a subset of people</span>
<span class="n">n_faces</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">face_ids</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_faces</span><span class="p">,))</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">face_ids</span><span class="p">,</span> <span class="p">:]</span>

<span class="n">n_pixels</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># Upper half of the faces</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[:,</span> <span class="p">:</span> <span class="p">(</span><span class="n">n_pixels</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>
<span class="c1"># Lower half of the faces</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[:,</span> <span class="n">n_pixels</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[:,</span> <span class="p">:</span> <span class="p">(</span><span class="n">n_pixels</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[:,</span> <span class="n">n_pixels</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>

<span class="c1"># Fit estimators</span>
<span class="n">ESTIMATORS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Extra trees&quot;</span><span class="p">:</span> <span class="n">ExtraTreesRegressor</span><span class="p">(</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">),</span>
    <span class="s2">&quot;K-nn&quot;</span><span class="p">:</span> <span class="n">KNeighborsRegressor</span><span class="p">(),</span>
    <span class="s2">&quot;Linear regression&quot;</span><span class="p">:</span> <span class="n">LinearRegression</span><span class="p">(),</span>
    <span class="s2">&quot;Ridge&quot;</span><span class="p">:</span> <span class="n">RidgeCV</span><span class="p">(),</span>
<span class="p">}</span>

<span class="n">y_test_predict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="n">ESTIMATORS</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test_predict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Plot the completed faces</span>
<span class="n">image_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

<span class="n">n_cols</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">ESTIMATORS</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">n_cols</span><span class="p">,</span> <span class="mf">2.26</span> <span class="o">*</span> <span class="n">n_faces</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Face completion with multi-output estimators&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_faces</span><span class="p">):</span>
    <span class="n">true_face</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="k">if</span> <span class="n">i</span><span class="p">:</span>
        <span class="n">sub</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_faces</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">n_cols</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sub</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_faces</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">n_cols</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;true faces&quot;</span><span class="p">)</span>

    <span class="n">sub</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
    <span class="n">sub</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
        <span class="n">true_face</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">image_shape</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">est</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">ESTIMATORS</span><span class="p">)):</span>
        <span class="n">completed_face</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_test_predict</span><span class="p">[</span><span class="n">est</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>

        <span class="k">if</span> <span class="n">i</span><span class="p">:</span>
            <span class="n">sub</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_faces</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">n_cols</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">j</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">sub</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_faces</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">n_cols</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">j</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">est</span><span class="p">)</span>

        <span class="n">sub</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
        <span class="n">sub</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
            <span class="n">completed_face</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">image_shape</span><span class="p">),</span>
            <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">,</span>
            <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">,</span>
        <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Neareast_Neighbors_44_0.png" src="../_images/Neareast_Neighbors_44_0.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./supervisados"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../introduccion.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Introducción</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="supportVectorMachine.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Support Vector Machines (SVM).</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Miguel Rodríguez<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>